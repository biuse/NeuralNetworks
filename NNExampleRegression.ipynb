{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos a entrenar una red para que sume 3 valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000,) (1000, 3)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#training set, en este caso lo simulamos. Generamos 3 n√∫meros aleatorios X1, X2 y X3 (inputs) y los sumamos (Y, output) \n",
    "X1=np.random.uniform(size=1000)*100\n",
    "X2=np.random.uniform(size=1000)*100\n",
    "X3=np.random.uniform(size=1000)*100\n",
    "X=np.transpose([X1,X2,X3])\n",
    "Y=X1+X2+X3\n",
    "print np.shape(Y),np.shape(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y) \n",
    "# separa el set total entre train y test (3/4 y 1/4 por default) . Exactamente lo mismo que hacer:\n",
    "#ntrain=3*len(y)/4\n",
    "#X_train=X[:ntrain,:];y_train=y[:ntrain]\n",
    "#X_test=X[ntrain:,:];y_test=y[ntrain:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "# definimos numero de capas y de hidden nodes (3 capas, de 2 ,5  y 10 neuronas) \n",
    "# Exagerado para este problema ..\n",
    "mlp = MLPRegressor(hidden_layer_sizes=(10),max_iter=500,verbose=True) # batch_size default depending on minimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 13958.78049504\n",
      "Iteration 2, loss = 13744.05879556\n",
      "Iteration 3, loss = 13532.45068293\n",
      "Iteration 4, loss = 13325.43746296\n",
      "Iteration 5, loss = 13128.50900055\n",
      "Iteration 6, loss = 12936.47275920\n",
      "Iteration 7, loss = 12747.91867480\n",
      "Iteration 8, loss = 12567.22701159\n",
      "Iteration 9, loss = 12392.24352866\n",
      "Iteration 10, loss = 12222.08716936\n",
      "Iteration 11, loss = 12062.13081155\n",
      "Iteration 12, loss = 11906.34024616\n",
      "Iteration 13, loss = 11755.72006466\n",
      "Iteration 14, loss = 11606.77437994\n",
      "Iteration 15, loss = 11465.03631219\n",
      "Iteration 16, loss = 11327.33430640\n",
      "Iteration 17, loss = 11191.37198920\n",
      "Iteration 18, loss = 11056.02540764\n",
      "Iteration 19, loss = 10925.66554242\n",
      "Iteration 20, loss = 10794.70348989\n",
      "Iteration 21, loss = 10665.92052116\n",
      "Iteration 22, loss = 10536.11925851\n",
      "Iteration 23, loss = 10410.28500710\n",
      "Iteration 24, loss = 10286.31997551\n",
      "Iteration 25, loss = 10161.72587092\n",
      "Iteration 26, loss = 10037.28066353\n",
      "Iteration 27, loss = 9912.46121984\n",
      "Iteration 28, loss = 9790.75087144\n",
      "Iteration 29, loss = 9666.72939350\n",
      "Iteration 30, loss = 9542.52878512\n",
      "Iteration 31, loss = 9416.79224324\n",
      "Iteration 32, loss = 9290.42540267\n",
      "Iteration 33, loss = 9163.04195820\n",
      "Iteration 34, loss = 9034.26651672\n",
      "Iteration 35, loss = 8901.84504040\n",
      "Iteration 36, loss = 8769.31031965\n",
      "Iteration 37, loss = 8632.52872678\n",
      "Iteration 38, loss = 8492.35030150\n",
      "Iteration 39, loss = 8351.80049381\n",
      "Iteration 40, loss = 8208.81918113\n",
      "Iteration 41, loss = 8065.17339799\n",
      "Iteration 42, loss = 7922.10671244\n",
      "Iteration 43, loss = 7771.11275476\n",
      "Iteration 44, loss = 7624.04861487\n",
      "Iteration 45, loss = 7471.51158731\n",
      "Iteration 46, loss = 7319.31208823\n",
      "Iteration 47, loss = 7160.97734135\n",
      "Iteration 48, loss = 7004.58419412\n",
      "Iteration 49, loss = 6847.36781038\n",
      "Iteration 50, loss = 6687.54517680\n",
      "Iteration 51, loss = 6530.36948160\n",
      "Iteration 52, loss = 6368.63688542\n",
      "Iteration 53, loss = 6207.16113623\n",
      "Iteration 54, loss = 6048.35774280\n",
      "Iteration 55, loss = 5886.59472342\n",
      "Iteration 56, loss = 5725.29839560\n",
      "Iteration 57, loss = 5564.41075896\n",
      "Iteration 58, loss = 5401.49616699\n",
      "Iteration 59, loss = 5244.37998694\n",
      "Iteration 60, loss = 5084.47260394\n",
      "Iteration 61, loss = 4929.46693586\n",
      "Iteration 62, loss = 4771.62243690\n",
      "Iteration 63, loss = 4618.94313464\n",
      "Iteration 64, loss = 4465.64902213\n",
      "Iteration 65, loss = 4317.10655521\n",
      "Iteration 66, loss = 4169.92831130\n",
      "Iteration 67, loss = 4024.39617419\n",
      "Iteration 68, loss = 3877.75110154\n",
      "Iteration 69, loss = 3739.02060881\n",
      "Iteration 70, loss = 3597.08791783\n",
      "Iteration 71, loss = 3458.56677181\n",
      "Iteration 72, loss = 3324.49445148\n",
      "Iteration 73, loss = 3193.70630199\n",
      "Iteration 74, loss = 3064.84825407\n",
      "Iteration 75, loss = 2939.30413356\n",
      "Iteration 76, loss = 2818.86252224\n",
      "Iteration 77, loss = 2702.09954931\n",
      "Iteration 78, loss = 2586.71740974\n",
      "Iteration 79, loss = 2476.51987891\n",
      "Iteration 80, loss = 2370.33372255\n",
      "Iteration 81, loss = 2266.30639302\n",
      "Iteration 82, loss = 2167.44174076\n",
      "Iteration 83, loss = 2070.87652707\n",
      "Iteration 84, loss = 1977.31888953\n",
      "Iteration 85, loss = 1888.96408924\n",
      "Iteration 86, loss = 1803.96516646\n",
      "Iteration 87, loss = 1723.19075756\n",
      "Iteration 88, loss = 1644.68462046\n",
      "Iteration 89, loss = 1571.06783636\n",
      "Iteration 90, loss = 1502.60439738\n",
      "Iteration 91, loss = 1436.20961089\n",
      "Iteration 92, loss = 1371.46113090\n",
      "Iteration 93, loss = 1312.85318950\n",
      "Iteration 94, loss = 1255.24778509\n",
      "Iteration 95, loss = 1201.03027318\n",
      "Iteration 96, loss = 1151.06314269\n",
      "Iteration 97, loss = 1102.04061673\n",
      "Iteration 98, loss = 1056.28405222\n",
      "Iteration 99, loss = 1015.39922138\n",
      "Iteration 100, loss = 974.58669591\n",
      "Iteration 101, loss = 937.24060995\n",
      "Iteration 102, loss = 902.58909447\n",
      "Iteration 103, loss = 869.13296292\n",
      "Iteration 104, loss = 838.06767013\n",
      "Iteration 105, loss = 809.73514749\n",
      "Iteration 106, loss = 782.85062040\n",
      "Iteration 107, loss = 758.52283147\n",
      "Iteration 108, loss = 734.97938705\n",
      "Iteration 109, loss = 714.54824968\n",
      "Iteration 110, loss = 694.29386529\n",
      "Iteration 111, loss = 676.48797685\n",
      "Iteration 112, loss = 659.56123192\n",
      "Iteration 113, loss = 643.43236715\n",
      "Iteration 114, loss = 628.55833030\n",
      "Iteration 115, loss = 615.79761439\n",
      "Iteration 116, loss = 602.62164529\n",
      "Iteration 117, loss = 591.18693080\n",
      "Iteration 118, loss = 579.22619735\n",
      "Iteration 119, loss = 568.83464491\n",
      "Iteration 120, loss = 558.78428277\n",
      "Iteration 121, loss = 549.55132036\n",
      "Iteration 122, loss = 540.57416329\n",
      "Iteration 123, loss = 531.98542778\n",
      "Iteration 124, loss = 523.60284024\n",
      "Iteration 125, loss = 515.71653357\n",
      "Iteration 126, loss = 508.16501511\n",
      "Iteration 127, loss = 500.63263139\n",
      "Iteration 128, loss = 493.49281480\n",
      "Iteration 129, loss = 486.50694899\n",
      "Iteration 130, loss = 479.68181554\n",
      "Iteration 131, loss = 473.31729700\n",
      "Iteration 132, loss = 466.51097960\n",
      "Iteration 133, loss = 460.08061081\n",
      "Iteration 134, loss = 453.95823553\n",
      "Iteration 135, loss = 447.58211715\n",
      "Iteration 136, loss = 441.66192776\n",
      "Iteration 137, loss = 435.43577221\n",
      "Iteration 138, loss = 429.50474227\n",
      "Iteration 139, loss = 423.66568964\n",
      "Iteration 140, loss = 417.70027922\n",
      "Iteration 141, loss = 411.89842810\n",
      "Iteration 142, loss = 406.08396532\n",
      "Iteration 143, loss = 400.30320030\n",
      "Iteration 144, loss = 394.44354113\n",
      "Iteration 145, loss = 389.00676910\n",
      "Iteration 146, loss = 383.07254547\n",
      "Iteration 147, loss = 377.60375503\n",
      "Iteration 148, loss = 371.82774603\n",
      "Iteration 149, loss = 366.27575529\n",
      "Iteration 150, loss = 360.78165855\n",
      "Iteration 151, loss = 355.20507680\n",
      "Iteration 152, loss = 349.60402177\n",
      "Iteration 153, loss = 343.95648662\n",
      "Iteration 154, loss = 338.16692821\n",
      "Iteration 155, loss = 332.73225149\n",
      "Iteration 156, loss = 326.83104431\n",
      "Iteration 157, loss = 321.03841833\n",
      "Iteration 158, loss = 315.01706431\n",
      "Iteration 159, loss = 308.50083146\n",
      "Iteration 160, loss = 302.35537422\n",
      "Iteration 161, loss = 295.18326423\n",
      "Iteration 162, loss = 288.74063420\n",
      "Iteration 163, loss = 281.50621820\n",
      "Iteration 164, loss = 274.30409136\n",
      "Iteration 165, loss = 266.44426521\n",
      "Iteration 166, loss = 258.18531896\n",
      "Iteration 167, loss = 250.01749058\n",
      "Iteration 168, loss = 240.47822212\n",
      "Iteration 169, loss = 230.87916722\n",
      "Iteration 170, loss = 221.04816470\n",
      "Iteration 171, loss = 210.77218932\n",
      "Iteration 172, loss = 201.09781824\n",
      "Iteration 173, loss = 191.05408479\n",
      "Iteration 174, loss = 181.90651119\n",
      "Iteration 175, loss = 173.16793310\n",
      "Iteration 176, loss = 164.88119306\n",
      "Iteration 177, loss = 156.98113740\n",
      "Iteration 178, loss = 149.77295412\n",
      "Iteration 179, loss = 143.12369443\n",
      "Iteration 180, loss = 136.85186032\n",
      "Iteration 181, loss = 131.06961050\n",
      "Iteration 182, loss = 125.75018955\n",
      "Iteration 183, loss = 120.22180673\n",
      "Iteration 184, loss = 115.17591599\n",
      "Iteration 185, loss = 110.20263724\n",
      "Iteration 186, loss = 105.62287250\n",
      "Iteration 187, loss = 101.16285586\n",
      "Iteration 188, loss = 96.95886936\n",
      "Iteration 189, loss = 93.06712308\n",
      "Iteration 190, loss = 89.38817436\n",
      "Iteration 191, loss = 85.75019684\n",
      "Iteration 192, loss = 82.39104681\n",
      "Iteration 193, loss = 79.09075703\n",
      "Iteration 194, loss = 76.10284710\n",
      "Iteration 195, loss = 73.17699223\n",
      "Iteration 196, loss = 70.34971682\n",
      "Iteration 197, loss = 67.79314890\n",
      "Iteration 198, loss = 65.32637233\n",
      "Iteration 199, loss = 62.88427431\n",
      "Iteration 200, loss = 60.67359174\n",
      "Iteration 201, loss = 58.47498855\n",
      "Iteration 202, loss = 56.47455103\n",
      "Iteration 203, loss = 54.59811128\n",
      "Iteration 204, loss = 52.70571743\n",
      "Iteration 205, loss = 50.97422042\n",
      "Iteration 206, loss = 49.31007782\n",
      "Iteration 207, loss = 47.71677288\n",
      "Iteration 208, loss = 46.15713985\n",
      "Iteration 209, loss = 44.73032087\n",
      "Iteration 210, loss = 43.38412133\n",
      "Iteration 211, loss = 42.05240568\n",
      "Iteration 212, loss = 40.83509933\n",
      "Iteration 213, loss = 39.60160692\n",
      "Iteration 214, loss = 38.45513786\n",
      "Iteration 215, loss = 37.41202333\n",
      "Iteration 216, loss = 36.32917482\n",
      "Iteration 217, loss = 35.35935829\n",
      "Iteration 218, loss = 34.48146214\n",
      "Iteration 219, loss = 33.49857057\n",
      "Iteration 220, loss = 32.62067409\n",
      "Iteration 221, loss = 31.76131001\n",
      "Iteration 222, loss = 30.99984573\n",
      "Iteration 223, loss = 30.24433495\n",
      "Iteration 224, loss = 29.50645589\n",
      "Iteration 225, loss = 28.81654047\n",
      "Iteration 226, loss = 28.13490084\n",
      "Iteration 227, loss = 27.46556652\n",
      "Iteration 228, loss = 26.89236480\n",
      "Iteration 229, loss = 26.27647539\n",
      "Iteration 230, loss = 25.67819412\n",
      "Iteration 231, loss = 25.17536832\n",
      "Iteration 232, loss = 24.62741820\n",
      "Iteration 233, loss = 24.12287446\n",
      "Iteration 234, loss = 23.62946142\n",
      "Iteration 235, loss = 23.18460213\n",
      "Iteration 236, loss = 22.72633011\n",
      "Iteration 237, loss = 22.27712350\n",
      "Iteration 238, loss = 21.85359805\n",
      "Iteration 239, loss = 21.43181616\n",
      "Iteration 240, loss = 21.05198218\n",
      "Iteration 241, loss = 20.67669979\n",
      "Iteration 242, loss = 20.31192363\n",
      "Iteration 243, loss = 19.97033718\n",
      "Iteration 244, loss = 19.61009929\n",
      "Iteration 245, loss = 19.29045262\n",
      "Iteration 246, loss = 18.98890038\n",
      "Iteration 247, loss = 18.65330243\n",
      "Iteration 248, loss = 18.35255052\n",
      "Iteration 249, loss = 18.07750869\n",
      "Iteration 250, loss = 17.80296675\n",
      "Iteration 251, loss = 17.52118613\n",
      "Iteration 252, loss = 17.24086017\n",
      "Iteration 253, loss = 17.00265717\n",
      "Iteration 254, loss = 16.73548732\n",
      "Iteration 255, loss = 16.51278350\n",
      "Iteration 256, loss = 16.27319707\n",
      "Iteration 257, loss = 16.06178648\n",
      "Iteration 258, loss = 15.82378591\n",
      "Iteration 259, loss = 15.62292307\n",
      "Iteration 260, loss = 15.42475478\n",
      "Iteration 261, loss = 15.20869903\n",
      "Iteration 262, loss = 15.01987075\n",
      "Iteration 263, loss = 14.84788938\n",
      "Iteration 264, loss = 14.66438349\n",
      "Iteration 265, loss = 14.48686262\n",
      "Iteration 266, loss = 14.31627316\n",
      "Iteration 267, loss = 14.14941850\n",
      "Iteration 268, loss = 13.98234363\n",
      "Iteration 269, loss = 13.84115950\n",
      "Iteration 270, loss = 13.68007791\n",
      "Iteration 271, loss = 13.53184288\n",
      "Iteration 272, loss = 13.38127189\n",
      "Iteration 273, loss = 13.24694240\n",
      "Iteration 274, loss = 13.10893821\n",
      "Iteration 275, loss = 12.96422508\n",
      "Iteration 276, loss = 12.83416060\n",
      "Iteration 277, loss = 12.70853880\n",
      "Iteration 278, loss = 12.58166949\n",
      "Iteration 279, loss = 12.45655273\n",
      "Iteration 280, loss = 12.33303694\n",
      "Iteration 281, loss = 12.22166512\n",
      "Iteration 282, loss = 12.10405919\n",
      "Iteration 283, loss = 11.98747874\n",
      "Iteration 284, loss = 11.88513050\n",
      "Iteration 285, loss = 11.76855935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 286, loss = 11.66269182\n",
      "Iteration 287, loss = 11.55589276\n",
      "Iteration 288, loss = 11.45585299\n",
      "Iteration 289, loss = 11.36130415\n",
      "Iteration 290, loss = 11.24911001\n",
      "Iteration 291, loss = 11.14955266\n",
      "Iteration 292, loss = 11.06110222\n",
      "Iteration 293, loss = 10.97019987\n",
      "Iteration 294, loss = 10.87407725\n",
      "Iteration 295, loss = 10.79551072\n",
      "Iteration 296, loss = 10.70747419\n",
      "Iteration 297, loss = 10.62267066\n",
      "Iteration 298, loss = 10.54637586\n",
      "Iteration 299, loss = 10.46645981\n",
      "Iteration 300, loss = 10.38858975\n",
      "Iteration 301, loss = 10.31527605\n",
      "Iteration 302, loss = 10.23561726\n",
      "Iteration 303, loss = 10.15447417\n",
      "Iteration 304, loss = 10.08778884\n",
      "Iteration 305, loss = 10.01599202\n",
      "Iteration 306, loss = 9.94203576\n",
      "Iteration 307, loss = 9.87418958\n",
      "Iteration 308, loss = 9.80322122\n",
      "Iteration 309, loss = 9.73572940\n",
      "Iteration 310, loss = 9.66308948\n",
      "Iteration 311, loss = 9.59937582\n",
      "Iteration 312, loss = 9.52888345\n",
      "Iteration 313, loss = 9.47109992\n",
      "Iteration 314, loss = 9.40375023\n",
      "Iteration 315, loss = 9.33675543\n",
      "Iteration 316, loss = 9.28169014\n",
      "Iteration 317, loss = 9.21201170\n",
      "Iteration 318, loss = 9.15270396\n",
      "Iteration 319, loss = 9.08826927\n",
      "Iteration 320, loss = 9.03197618\n",
      "Iteration 321, loss = 8.97086723\n",
      "Iteration 322, loss = 8.91829780\n",
      "Iteration 323, loss = 8.85959595\n",
      "Iteration 324, loss = 8.80264838\n",
      "Iteration 325, loss = 8.74753861\n",
      "Iteration 326, loss = 8.69454097\n",
      "Iteration 327, loss = 8.63995669\n",
      "Iteration 328, loss = 8.58839707\n",
      "Iteration 329, loss = 8.53629946\n",
      "Iteration 330, loss = 8.49385671\n",
      "Iteration 331, loss = 8.43791011\n",
      "Iteration 332, loss = 8.38656166\n",
      "Iteration 333, loss = 8.33489870\n",
      "Iteration 334, loss = 8.28714444\n",
      "Iteration 335, loss = 8.23918926\n",
      "Iteration 336, loss = 8.19420631\n",
      "Iteration 337, loss = 8.14975259\n",
      "Iteration 338, loss = 8.09929573\n",
      "Iteration 339, loss = 8.06588246\n",
      "Iteration 340, loss = 8.01309847\n",
      "Iteration 341, loss = 7.96768719\n",
      "Iteration 342, loss = 7.92237991\n",
      "Iteration 343, loss = 7.87894259\n",
      "Iteration 344, loss = 7.83456193\n",
      "Iteration 345, loss = 7.79372161\n",
      "Iteration 346, loss = 7.75302652\n",
      "Iteration 347, loss = 7.71093886\n",
      "Iteration 348, loss = 7.67724948\n",
      "Iteration 349, loss = 7.62990468\n",
      "Iteration 350, loss = 7.58904227\n",
      "Iteration 351, loss = 7.55058366\n",
      "Iteration 352, loss = 7.51076606\n",
      "Iteration 353, loss = 7.47372953\n",
      "Iteration 354, loss = 7.43743091\n",
      "Iteration 355, loss = 7.39744272\n",
      "Iteration 356, loss = 7.36183863\n",
      "Iteration 357, loss = 7.33043901\n",
      "Iteration 358, loss = 7.28979307\n",
      "Iteration 359, loss = 7.25410987\n",
      "Iteration 360, loss = 7.21943595\n",
      "Iteration 361, loss = 7.18802526\n",
      "Iteration 362, loss = 7.14887513\n",
      "Iteration 363, loss = 7.12016767\n",
      "Iteration 364, loss = 7.08221686\n",
      "Iteration 365, loss = 7.04719514\n",
      "Iteration 366, loss = 7.02016026\n",
      "Iteration 367, loss = 6.98173845\n",
      "Iteration 368, loss = 6.94744662\n",
      "Iteration 369, loss = 6.91538306\n",
      "Iteration 370, loss = 6.88231635\n",
      "Iteration 371, loss = 6.85279849\n",
      "Iteration 372, loss = 6.81988707\n",
      "Iteration 373, loss = 6.78687718\n",
      "Iteration 374, loss = 6.75613467\n",
      "Iteration 375, loss = 6.72567005\n",
      "Iteration 376, loss = 6.69624519\n",
      "Iteration 377, loss = 6.66890610\n",
      "Iteration 378, loss = 6.63727063\n",
      "Iteration 379, loss = 6.60825142\n",
      "Iteration 380, loss = 6.57864473\n",
      "Iteration 381, loss = 6.54980559\n",
      "Iteration 382, loss = 6.51990732\n",
      "Iteration 383, loss = 6.49159058\n",
      "Iteration 384, loss = 6.46418816\n",
      "Iteration 385, loss = 6.43636514\n",
      "Iteration 386, loss = 6.40853601\n",
      "Iteration 387, loss = 6.38378129\n",
      "Iteration 388, loss = 6.35277419\n",
      "Iteration 389, loss = 6.32999197\n",
      "Iteration 390, loss = 6.30401189\n",
      "Iteration 391, loss = 6.27533402\n",
      "Iteration 392, loss = 6.25160155\n",
      "Iteration 393, loss = 6.22648117\n",
      "Iteration 394, loss = 6.20094801\n",
      "Iteration 395, loss = 6.17974034\n",
      "Iteration 396, loss = 6.15140224\n",
      "Iteration 397, loss = 6.12339971\n",
      "Iteration 398, loss = 6.10080786\n",
      "Iteration 399, loss = 6.07623516\n",
      "Iteration 400, loss = 6.05183289\n",
      "Iteration 401, loss = 6.02839238\n",
      "Iteration 402, loss = 6.00338989\n",
      "Iteration 403, loss = 5.97882862\n",
      "Iteration 404, loss = 5.95481899\n",
      "Iteration 405, loss = 5.93573193\n",
      "Iteration 406, loss = 5.90932346\n",
      "Iteration 407, loss = 5.89821925\n",
      "Iteration 408, loss = 5.86492940\n",
      "Iteration 409, loss = 5.84097922\n",
      "Iteration 410, loss = 5.82090548\n",
      "Iteration 411, loss = 5.79874107\n",
      "Iteration 412, loss = 5.77552995\n",
      "Iteration 413, loss = 5.75730389\n",
      "Iteration 414, loss = 5.73469580\n",
      "Iteration 415, loss = 5.71017795\n",
      "Iteration 416, loss = 5.68917973\n",
      "Iteration 417, loss = 5.66955072\n",
      "Iteration 418, loss = 5.64707445\n",
      "Iteration 419, loss = 5.62809521\n",
      "Iteration 420, loss = 5.61065491\n",
      "Iteration 421, loss = 5.58595928\n",
      "Iteration 422, loss = 5.56310669\n",
      "Iteration 423, loss = 5.54337852\n",
      "Iteration 424, loss = 5.52319772\n",
      "Iteration 425, loss = 5.50243284\n",
      "Iteration 426, loss = 5.48030470\n",
      "Iteration 427, loss = 5.46197935\n",
      "Iteration 428, loss = 5.44300147\n",
      "Iteration 429, loss = 5.42108698\n",
      "Iteration 430, loss = 5.40254762\n",
      "Iteration 431, loss = 5.38466456\n",
      "Iteration 432, loss = 5.36428200\n",
      "Iteration 433, loss = 5.34523880\n",
      "Iteration 434, loss = 5.32657670\n",
      "Iteration 435, loss = 5.30690894\n",
      "Iteration 436, loss = 5.29013552\n",
      "Iteration 437, loss = 5.27372085\n",
      "Iteration 438, loss = 5.25475946\n",
      "Iteration 439, loss = 5.23495921\n",
      "Iteration 440, loss = 5.21806213\n",
      "Iteration 441, loss = 5.19919621\n",
      "Iteration 442, loss = 5.18236173\n",
      "Iteration 443, loss = 5.16725121\n",
      "Iteration 444, loss = 5.14428022\n",
      "Iteration 445, loss = 5.13186275\n",
      "Iteration 446, loss = 5.11109469\n",
      "Iteration 447, loss = 5.09529603\n",
      "Iteration 448, loss = 5.07557500\n",
      "Iteration 449, loss = 5.05808935\n",
      "Iteration 450, loss = 5.04167382\n",
      "Iteration 451, loss = 5.02678468\n",
      "Iteration 452, loss = 5.01232927\n",
      "Iteration 453, loss = 4.99212115\n",
      "Iteration 454, loss = 4.97639067\n",
      "Iteration 455, loss = 4.95900731\n",
      "Iteration 456, loss = 4.94342781\n",
      "Iteration 457, loss = 4.92674104\n",
      "Iteration 458, loss = 4.90878084\n",
      "Iteration 459, loss = 4.89226596\n",
      "Iteration 460, loss = 4.87513599\n",
      "Iteration 461, loss = 4.85860396\n",
      "Iteration 462, loss = 4.84310516\n",
      "Iteration 463, loss = 4.82801086\n",
      "Iteration 464, loss = 4.80879472\n",
      "Iteration 465, loss = 4.79420609\n",
      "Iteration 466, loss = 4.78052732\n",
      "Iteration 467, loss = 4.76126574\n",
      "Iteration 468, loss = 4.74467374\n",
      "Iteration 469, loss = 4.72839977\n",
      "Iteration 470, loss = 4.71151068\n",
      "Iteration 471, loss = 4.69713132\n",
      "Iteration 472, loss = 4.68169876\n",
      "Iteration 473, loss = 4.66202366\n",
      "Iteration 474, loss = 4.64656100\n",
      "Iteration 475, loss = 4.63246580\n",
      "Iteration 476, loss = 4.61462603\n",
      "Iteration 477, loss = 4.59839160\n",
      "Iteration 478, loss = 4.58489509\n",
      "Iteration 479, loss = 4.56826139\n",
      "Iteration 480, loss = 4.55099353\n",
      "Iteration 481, loss = 4.53540851\n",
      "Iteration 482, loss = 4.52111627\n",
      "Iteration 483, loss = 4.50448478\n",
      "Iteration 484, loss = 4.48771119\n",
      "Iteration 485, loss = 4.47574938\n",
      "Iteration 486, loss = 4.45992350\n",
      "Iteration 487, loss = 4.44408376\n",
      "Iteration 488, loss = 4.42706467\n",
      "Iteration 489, loss = 4.41328755\n",
      "Iteration 490, loss = 4.39980253\n",
      "Iteration 491, loss = 4.38334136\n",
      "Iteration 492, loss = 4.36869564\n",
      "Iteration 493, loss = 4.35393031\n",
      "Iteration 494, loss = 4.33768547\n",
      "Iteration 495, loss = 4.32178552\n",
      "Iteration 496, loss = 4.30916726\n",
      "Iteration 497, loss = 4.29600596\n",
      "Iteration 498, loss = 4.27707208\n",
      "Iteration 499, loss = 4.26287840\n",
      "Iteration 500, loss = 4.25073519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=10, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=500, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp.fit(X_train,y_train) # usa modelo red neuronal que hemos definido antes y(x,w)\n",
    "# mlp.loss_curve en Clasificaci√≥n te guarda el valor de la funci√≥n coste a cada iteraci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.99967216]\n",
      " [0.99967216 1.        ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x10e5b3410>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test) # estimamos resultado con el TEST\n",
    "print np.corrcoef(predictions,y_test)  # calculamos correlaci√≥n con el valor esperado\n",
    "import matplotlib.pyplot as pl\n",
    "pl.plot(predictions,y_test,'.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le pregunto a la red que me sume 10+30+5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la suma es 47.0\n"
     ]
    }
   ],
   "source": [
    "A=np.array([10,30,5]).reshape(1,-1) # son solo un vector necesita que se lo de dimension(n,1) (sklearn !! )\n",
    "np.shape(A)\n",
    "print 'la suma es', round(mlp.predict(A)) # redondamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si prob√°is distinto n√∫mero ver√©is que hacemos errores en los valores peque√±os, el error promedio que hemos \n",
    "#conseguido afecta m√°s a los n√∫meros peque√±os, no es lo mismo tener un error de 2 en 2+2 que en 100+200. Se puede\n",
    "# usar una funci√≥n objetivo que sea la suma cuadr√°tica de errores relativos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
